{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request as req\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extraction(content):\n",
    "    name = []\n",
    "    location = []\n",
    "    date = []\n",
    "    who = re.findall(r'itemprop=\"author\">(.+?)</span>',content)\n",
    "    for n in who:\n",
    "        breakdown = re.match(r'(.+?) of (.+)',n)\n",
    "        name.append(breakdown.group(1))\n",
    "        location.append(breakdown.group(2))\n",
    "        \n",
    "    when = re.findall(r'<span class=\"review__post-date\" content=\"(.+?)\"',content)\n",
    "    for t in when:\n",
    "        breakdown = re.match(r'(\\d{4}\\-\\d{2}-\\d{2}) (\\d{2}:\\d{2}:\\d{2})',t)\n",
    "        date.append(pandas.to_datetime(breakdown.group(0),format='%Y-%m-%d %H:%M:%S'))\n",
    "    what = re.findall(r'<div class=\"non-brand-campaign clearfix\">(.*?)(<div class=\"\">)*(<p>)*(.+?)</p>(<p>)*</div>',content)\n",
    "    for x in range(len(what)):\n",
    "        what[x] = what[x][3]\n",
    "        what[x] = what[x].strip()\n",
    "        what[x] = re.sub('<(.+?)>','',what[x])\n",
    "    \n",
    "    \n",
    "    rating = re.findall(r'itemprop=\"ratingValue\" content=\"([1-5])\"',content)\n",
    "    rating = rating = [int(x) for x in rating]\n",
    "    rating = rating + [None for x in range(len(name) - len(rating))]\n",
    "    \n",
    "    data = {'name':name,'location':location,'datetime':date,'review':what,'rating':rating}\n",
    "    result = pandas.DataFrame(data)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseurl = 'https://www.consumeraffairs.com/finance/sallie_mae.html?page='\n",
    "page = 1\n",
    "\n",
    "resp = req.urlopen(baseurl + str(page))\n",
    "webpage = resp.read()\n",
    "webpage_fixed = webpage.decode('utf-8')\n",
    "webpage_fixed = webpage_fixed.replace('\\n','')\n",
    "result = extraction(webpage_fixed)\n",
    "page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished page 2, Trying page 3\n",
      "Finished page 3, Trying page 4\n",
      "Finished page 4, Trying page 5\n",
      "Finished page 5, Trying page 6\n",
      "Finished page 6, Trying page 7\n",
      "Finished page 7, Trying page 8\n",
      "Finished page 8, Trying page 9\n",
      "Finished page 9, Trying page 10\n",
      "Finished page 10, Trying page 11\n",
      "Finished page 11, Trying page 12\n",
      "Finished page 12, Trying page 13\n",
      "Finished page 13, Trying page 14\n",
      "Finished page 14, Trying page 15\n",
      "Finished page 15, Trying page 16\n",
      "Finished page 16, Trying page 17\n",
      "Finished page 17, Trying page 18\n",
      "Finished page 18, Trying page 19\n",
      "Finished page 19, Trying page 20\n",
      "Finished page 20, Trying page 21\n",
      "Finished page 21, Trying page 22\n",
      "Finished page 22, Trying page 23\n",
      "Finished page 23, Trying page 24\n",
      "Finished page 24, Trying page 25\n",
      "Finished page 25, Trying page 26\n",
      "Finished page 26, Trying page 27\n",
      "Finished page 27, Trying page 28\n",
      "Finished page 28, Trying page 29\n",
      "Finished page 29, Trying page 30\n",
      "Finished page 30, Trying page 31\n",
      "Finished page 31, Trying page 32\n",
      "Finished page 32, Trying page 33\n",
      "Finished page 33, Trying page 34\n",
      "Finished page 34, Trying page 35\n",
      "Finished page 35, Trying page 36\n",
      "Finished page 36, Trying page 37\n",
      "Finished page 37, Trying page 38\n",
      "Finished page 38, Trying page 39\n",
      "Finished page 39, Trying page 40\n",
      "Finished page 40, Trying page 41\n",
      "Finished page 41, Trying page 42\n",
      "Finished page 42, Trying page 43\n",
      "Finished page 43, Trying page 44\n",
      "Finished page 44, Trying page 45\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    resp = req.urlopen(baseurl + str(page))\n",
    "    actual_page = int(re.search(r'page=(\\d+)',resp.url).groups(0)[0])\n",
    "    if(page != actual_page):\n",
    "        break\n",
    "    webpage = resp.read()\n",
    "    webpage_fixed = webpage.decode('utf-8')\n",
    "    webpage_fixed = webpage_fixed.replace('\\n','')\n",
    "    extracted = extraction(webpage_fixed)\n",
    "    result = pandas.concat([result,extracted],axis=0)\n",
    "    print('Finished page ' + str(page) + ', Trying page ' + str(page+1))\n",
    "    page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result.reset_index(inplace=True,drop=True)\n",
    "result.to_csv('consumer_affairs_sallie_mae.csv',index=False)\n",
    "result.to_pickle('consumer_affairs_sallie_mae.data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
